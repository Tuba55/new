Prac:1-Computing Summary Statistics using real time data 

library(tidyverse)

# Load iris dataset
head(iris)
summary(iris)

# Convert to tibble data frame
my_data <- as_tibble(iris)
my_data

# Summary statistics of ungrouped data
# Compute the mean of Sepal.Length and Petal.Length as well as the number of observations using the function n()
my_data %>%
  summarize(
    count = n(),
    mean_sep = mean(Sepal.Length, na.rm = TRUE),
    mean_pet = mean(Petal.Length, na.rm = TRUE)
  )

# Summary Statistics of grouped data
# Group by one variable
my_data %>%
  group_by(Species) %>%
  summarize(
    count = n(),
    mean_sep = mean(Sepal.Length),
    mean_pet = mean(Petal.Length)
  )

# Summarize all variables - compute the mean of all variables
my_data %>%
  group_by(Species) %>%
  summarise_all(mean)

# Median and standard deviation
my_data %>%
  group_by(Species) %>%
  summarize(
    count = n(),
    median_sep = median(Sepal.Length),
    sdev_pet = sd(Petal.Length)
  )

# Inter-quantile range IQR
my_data %>%
  group_by(Species) %>%
  summarise(
    count = n(),
    IQR_sep = IQR(Sepal.Length),
    IQR_pet = IQR(Petal.Length)
  )
Prac 2:Applying stepwise linear regression models to real dataset and interpreting the coefficient of determination for scale data (Done)

# Initialize an empty model 
forward_model <- lm(mpg ~ ., data = mtcars) # Forward stepwise regression 
forward_model <- step(forward_model, direction = "forward", scope = formula(~ .)) 

# Initialize a model with all predictors 
backward_model <- lm(mpg ~ ., data = mtcars) # Backward stepwise regression 
backward_model <- step(backward_model, direction = "backward") 

# Initialize a model with all predictors 
both_model <- lm(mpg ~ ., data = mtcars) # Both-direction stepwise regression 
both_model <- step(both_model, direction = "both", scope = formula(~ .)) 

# Scatter plot of mpg vs. hp
plot(mtcars$hp, mtcars$mpg, main = "Scatter Plot of mpg vs. hp", xlab = "hp", ylab = "mpg", pch = 20) 

abline(lm(mpg ~ hp, data = mtcars), col = "black", lwd = 2) 
points(sort(mtcars$hp), fitted(forward_model)[order(mtcars$hp)], col = "red", pch = 20) 
points(sort(mtcars$hp), fitted(backward_model)[order(mtcars$hp)], col = "blue", pch = 20) 
points(sort(mtcars$hp), fitted(both_model)[order(mtcars$hp)], col = "green", pch = 20) 

legend("topright", legend = c("Forward", "Backward", "Both-Direction"), col = c("red", "blue", "green"), pch = 20)

Prac3:Performing ANOVA (one way and two-way anova ) for real dataset 
# One-way ANOVA
data1 <- read.csv(file.choose(), sep = ",", header = TRUE)
names(data1)
summary(data1)
head(data1)
anv <- aov(satindex ~ dept, data = data1) # Corrected formula
summary(anv)

# Two-way ANOVA
data2 <- read.csv(file.choose(), sep = ",", header = TRUE)
names(data2)
summary(data2)
anv1 <- aov(satindex ~ dept + exp + dept * exp, data = data2) # Corrected formula
summary(anv1)

pract4:Testing of hypothesis for small sample tests for One and Two Sample mean and paired comparison 

# 1. Sample t-test on apple sales (assuming data in C1 column of a CSV file named "apple_data.csv")
data <- read.csv("apple_data.csv")
t.test(data$C1, mu = 97, alternative = "less")

# 2. Independent t-test on salary (assuming data in MALES and FEMALES columns of a CSV file named "salary_data.csv")
data <- read.csv("salary_data.csv")
t.test(data$MALES, data$FEMALES, var.equal = TRUE)

# 3. Paired t-test on poverty level (assuming data in X1 and X2 columns of a CSV file named "poverty_data.csv")
data <- read.csv("poverty_data.csv")
t.test(data$X1, data$X2, paired = TRUE, alternative = "greater")

# 4. Paired t-test on time (assuming data in time_before and time_after columns of a CSV file named "time_data.csv")
data <- read.csv("time_data.csv")
t.test(data$time_before, data$time_after, paired = TRUE, alternative = "less")

pract:5-Testing of hypothesis for Small Sample tests for F-test 
# F test for variance
var <- read.csv(file.choose(), sep = ",", header = TRUE)

summary(var)
var.test(var$time_g1, var$time_g2, alternative = "two.sided")

 Pract 6:Implementation of Logistic Regression 
# Load libraries
library(MASS)

# Load data
data(biopsy)

# Explore data
head(biopsy)
str(biopsy)

# Data cleaning
biopsy$ID <- NULL  # Remove ID variable (assuming it's not relevant)
colSums(is.na(biopsy))  # Check for missing values

# Exclude rows with missing values
biopsyl <- na.omit(biopsy)

# Split data into training and testing sets (70% training, 30% testing)
set.seed(123)  # Set seed for reproducible random split
ind <- sample(2, nrow(biopsyl), replace = TRUE, prob = c(0.7, 0.3))
train <- biopsyl[ind == 1, ]
test <- biopsyl[ind == 2, ]

# Feature engineering (add predicted probability from another model)
train$prob <- predict(reducefit, type = "response")  # Assuming reducefit is a model

# Label encoding target variable
train$predict <- rep("benign", 474)
train$predict[train$prob > 0.5] <- "malignant"

# Logistic regression model fitting
fullfit <- glm(class ~ ., family = binomial, data = train)

# Model evaluation
summary(fullfit)
exp(coef(fullfit))  # Calculate odds ratios

# Predictions on training and testing data
train$prob <- predict(fullfit, type = "response")
test$prob <- predict(fullfit, newdata = test, type = "response")
test$predict <- rep("benign", 209)
test$predict[test$prob > 0.5] <- "malignant"

Pract 7:Implement time series analysis with ARMA/ARIMA model. 

#Time series not use when value is constant
#Time series use when data is stationary condtion for stationary is mean is constant 
#with time and varience and co-varience.
end(AirPassengers)
start(AirPassengers)
plot(AirPassengers)
time(AirPassengers)

abline(reg =lm(AirPassengers~time(AirPassengers)))
#Fit the line:-view the mean and varience

summary(AirPassengers)
#Component of Time Series :- general Trends,Sesonal,Irregular Fluctuation
#General trends show where the data is increasing or decreasing
plot(aggregate(AirPassengers,FUN=mean))#it will show the trend of data where data is increasing or decreasing
aggregate(AirPassengers,FUN=mean)#Yearly mean of data
#Sesonal data show where there is any fluctuation in perticular data or not
#i.e how many pick in data
#Irregular Fluctuation :- the data give irrelavent pick that is not predictable

class(AirPassengers)
frequency(AirPassengers)#12 Monthes

boxplot(AirPassengers~cycle(AirPassengers))#shows where the is any seasonal attributes are not

input <- mtcars[,c('mpg','cyl')]
print(head(input))
boxplot(mpg ~ cyl, data = mtcars, xlab = "Number of Cylinders",
        ylab = "Miles Per Gallon", main = "Mileage Data")

#Data Serielazation process
#using log fuction we make varience constant
log(AirPassengers)
plot(log(AirPassengers))
#2)constant the mean  using diff() function
plot(diff(log(AirPassengers)))

#Time series analysis using AR I MA model(auto regression integration and moving average)
#AR-P ,I-d ,MA-q
help(acf)

#detemining q value using acf check which first line go down word and take one previous of that line and start numbering from 0
acf(diff(log(AirPassengers)))#here q is 1
pacf(diff(log(AirPassengers)))#here p value is 0 
#and d value is get by how much time we use diff function here is 1
fit <- arima(log(AirPassengers), c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12))
fit
pred<-predict(fit,n.ahead = 10*12)#10 year and 12 month prediction we have calculated
pred1<-2.718^pred$pred#Converting log value to decimal value we use e value


ts.plot(AirPassengers,pred1)

#Testing the model 
dataw<-ts(AirPassengers,frequency = 12,start = c(1949,1),end=c(1959,12))
fit<-arima(log(dataw),c(0,1,1),seasonal = list(order=c(0,1,1),period=12))#we don't have to take difference because we pass d value 1 have it automaticaly does it
pred1<-predict(fit,n.ahead = 10*12)
pred1<-2.718^pred1$pred
data1<-head(pred1,12)
predict_1960=round(data1,digits = 0)
original_1960=tail(AirPassengers,12)

Pract 8:Testing of hypothesis for large sample tests for real-time problems


